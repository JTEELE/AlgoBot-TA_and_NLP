{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This script is activated from 'main'. See respective script for NLP imports & dependencies\n",
        "print('Successfully activated NLP Script')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reddit: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('')\n",
        "coin_sentiment = []\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "#Initiate NLP Sentiment Function \n",
        "def article_sentiment(news):\n",
        "\n",
        "    for post in news['data']['children']:\n",
        "    \n",
        "        try:\n",
        "            sentiment = analyzer.polarity_scores(post['data']['selftext'])\n",
        "            compound = sentiment[\"compound\"]\n",
        "            pos = sentiment[\"pos\"]\n",
        "            neu = sentiment[\"neu\"]\n",
        "            neg = sentiment[\"neg\"]\n",
        "    \n",
        "            coin_sentiment.append({\n",
        "                'date':str(datetime.fromtimestamp(post['data']['created'])),\n",
        "                'subreddit':post['data']['subreddit'],\n",
        "                'title':post['data']['title'],\n",
        "                'selftext':post['data']['selftext'],\n",
        "                'upvote_ratio':post['data']['upvote_ratio'], \n",
        "                'ups':post['data']['ups'],\n",
        "                'downs':post['data']['downs'],\n",
        "                'score':post['data']['score'],\n",
        "                'compound': compound,\n",
        "                'positive': pos,\n",
        "                'negative': neg,\n",
        "                'neutral': neu})\n",
        "        except AttributeError:\n",
        "            pass\n",
        "    sentiment_df = pd.DataFrame(coin_sentiment)\n",
        "    return sentiment_df\n",
        "\n",
        "#Reddit Analysis\n",
        "#Extract Reddit NLP Data and write to csv file\n",
        "reddit_requests = {}\n",
        "\n",
        "for i in range(len(nlp_cryptos)):\n",
        "    response = requests.get(f'https://oauth.reddit.com/r/{nlp_cryptos[i]}/controversial.json?limit=10&t=day', headers=headers)\n",
        "    if response.status_code ==200:\n",
        "        reddit_requests[i] = response\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "\n",
        "sentiment_df = pd.DataFrame()\n",
        "for i in reddit_requests:\n",
        "    try:\n",
        "        articles = (reddit_requests[i].json())\n",
        "        sentiment_df = sentiment_df.append(article_sentiment(articles))\n",
        "    except:\n",
        "        pass\n",
        "sentiment_df['subreddit'] = sentiment_df['subreddit'].str.lower()\n",
        "\n",
        "isolation = dict()\n",
        "for k, v in sentiment_df.groupby('subreddit'):\n",
        "    isolation[k] = v\n",
        "\n",
        "summary = dict()\n",
        "for x in nlp_cryptos:\n",
        "    try:\n",
        "        summary[x] = (isolation[x].describe())\n",
        "        summary[x].to_csv(f'Data/Functionality/Reddit/Reddit_{x}.csv')\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Twitter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ILj1eTSAWI15"
      },
      "outputs": [],
      "source": [
        "#Twitter Analysis\n",
        "#Extract Tweets from financial influencers on twitter using Tweepy\n",
        "influencers = ['elonmusk','CoinMarketCap','Cointelegraph','Gemini','wallstreetbets','krakenfx','coinbase']\n",
        "\n",
        "#Extract 15 tweets from each twitter user timeline\n",
        "recent_twitter_df = pd.DataFrame()\n",
        "for influencer in influencers:\n",
        "    recent_posts = api.user_timeline(screen_name = influencer, count=10, tweet_mode='extended')\n",
        "    data = pd.DataFrame( [tweet.full_text for tweet in recent_posts] , columns=['Tweets'])\n",
        "    recent_twitter_df = recent_twitter_df.append(data)\n",
        "\n",
        "#Clean twitter posts for NLP Analaysis\n",
        "def clean_text(text):\n",
        "    text= re.sub(r'@[A-Za-z0-9]+', '', text) #removes @mentions\n",
        "    text = re.sub(r'#','', text) #removes the # symbol\n",
        "    text = re.sub(r'RT[\\s]+','', text) #removes RT\n",
        "    text = re.sub(r'https?:\\/\\/\\S+','', text) #removes hyperlink\n",
        "    return text\n",
        "\n",
        "recent_twitter_df['Tweets'] = recent_twitter_df['Tweets'].apply(clean_text)\n",
        "\n",
        "#create a function to get subjectivity and polarities\n",
        "#subjectivity - how opinionated the text is\n",
        "def get_subjectivity(text):\n",
        "    return TextBlob(text).sentiment.subjectivity\n",
        "    \n",
        "#create a function to get polarity, how positive or negative the text is\n",
        "def get_polarity(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "#create two new columns\n",
        "recent_twitter_df['Subjectivity'] = recent_twitter_df['Tweets'].apply(get_subjectivity)\n",
        "recent_twitter_df['Polarity'] = recent_twitter_df['Tweets'].apply(get_polarity)\n",
        "\n",
        "#create function to analyze polarity\n",
        "def get_analysis(score):\n",
        "    if score <0:\n",
        "        return 'Negative'\n",
        "    elif score == 0:\n",
        "        return 'Neutral'\n",
        "    else:\n",
        "        return 'Positive'\n",
        "    \n",
        "#create new column for analysis    \n",
        "recent_twitter_df['Analysis'] = recent_twitter_df['Polarity'].apply(get_analysis)\n",
        "recent_twitter_df.to_csv('Data/Functionality/Twitter/raw_twitter_data.csv')\n",
        "recent_twitter_df.to_csv('Data/Functionality/Twitter/twitter_clean_text_function.csv')\n",
        "\n",
        "#plot Word Cloud users timelines\n",
        "all_words = ' '.join( [twts for twts in recent_twitter_df['Tweets']] )\n",
        "word_cloud = WordCloud(width = 500, height=300, random_state = 2, max_font_size = 119).generate(all_words)\n",
        "\n",
        "plt.imshow(word_cloud, interpolation = 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "neUkVUabZV5u"
      },
      "outputs": [],
      "source": [
        "#Find general crypto sentiment using search terms on twitter - Uses Tweepy version 4\n",
        "#Bearer_Token Authentication\n",
        "bearer_token = os.getenv(\"bearer_token\")\n",
        "consumer_key = os.getenv(\"consumer_key\")\n",
        "consumer_secret_key = os.getenv(\"consumer_secret\")\n",
        "client = tweepy.Client(bearer_token)\n",
        "\n",
        "#Authenticate account using consumer_key & consumer_secret_key\n",
        "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret_key)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "#Create new API search for recent tweets using a search term, then perform tone analysis to get market sentiment\n",
        "query = 'crypto+market, -filter:retweets'\n",
        "max_tweets = 25\n",
        "posts = [status for status in tweepy.Cursor(api.search_tweets,q=query, lang='en', result_type='recent', count=25).items(max_tweets)]\n",
        "\n",
        "#Create array from results and create dataframe of results\n",
        "string_array = []\n",
        "for status in posts:\n",
        "    string_array.append(status.text)\n",
        "query_tweets_df = pd.DataFrame(string_array, columns=['Tweets'])\n",
        "query_tweets_df.to_csv('Data/Functionality/Twitter/query_tweets_raw.csv')\n",
        "\n",
        "#Clean_text to search query tweets\n",
        "query_tweets_df['Tweets'] = query_tweets_df['Tweets'].apply(clean_text)\n",
        "recent_twitter_df.to_csv('Data/Functionality/Twitter/users_subjectivity_and_polarity.csv')\n",
        "query_tweets_df.to_csv('Data/Functionality/Twitter/query_tweets_clean_text.csv')\n",
        "\n",
        "#Create new columns and apply subjectivity and polarity sentiment analysis\n",
        "query_tweets_df['Subjectivity'] = query_tweets_df['Tweets'].apply(get_subjectivity)\n",
        "query_tweets_df['Polarity'] = query_tweets_df['Tweets'].apply(get_polarity)\n",
        "\n",
        "#Create new column for analysis of polarity  \n",
        "query_tweets_df['Analysis'] = query_tweets_df['Polarity'].apply(get_analysis)\n",
        "\n",
        "#Plot word cloud to visualize tweets in conjunction with search query \n",
        "all_words = ' '.join( [twts for twts in query_tweets_df['Tweets']] )\n",
        "word_cloud = WordCloud(width = 500, height=300, random_state = 2, max_font_size = 119).generate(all_words)\n",
        "query_tweets_df.to_csv('Data/Functionality/Twitter/market_sentiment_analysis.csv')\n",
        "\n",
        "plt.imshow(word_cloud, interpolation = 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "#Plot the polarity and subjectivity \n",
        "plt.figure(figsize=(8,6))\n",
        "for i in range(0, query_tweets_df.shape[0]):\n",
        "    plt.scatter(query_tweets_df['Polarity'][i], query_tweets_df['Subjectivity'][i], color='blue' )\n",
        "plt.title('Sentiment Analysis')\n",
        "plt.xlabel('Polarity')\n",
        "plt.ylabel('Subjectivity')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0O-U9TdPyuOV"
      },
      "outputs": [],
      "source": [
        "#Authenticate IBM watson api connection & keys to use tone_analyzer\n",
        "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
        "tone_api = os.getenv(\"tone_api\")\n",
        "\n",
        "#Set_service_url is a private/personal endpoint url provided in your IBM-watson account, not publicly available\n",
        "authenticator = IAMAuthenticator(tone_api)\n",
        "tone_analyzer = ToneAnalyzerV3(\n",
        "    version='2017-09-21',\n",
        "    authenticator=authenticator\n",
        ")\n",
        "tone_analyzer.set_service_url('https://api.au-syd.tone-analyzer.watson.cloud.ibm.com/instances/85c20273-70b0-4277-b776-694e7146a612')\n",
        "\n",
        "#Create a document from query_tweets_df to run as single string through IBM tone_analyzer\n",
        "full_text = \" \".join( [tweets for tweets in query_tweets_df[\"Tweets\"]] )\n",
        "\n",
        "#Define text as string\n",
        "text = full_text\n",
        "\n",
        "# Analyze the text's tone with the 'tone()' method.\n",
        "tone_analysis = tone_analyzer.tone(\n",
        "    {\"text\": text},\n",
        "    content_type=\"application/json\",\n",
        "    content_language=\"en\",\n",
        "    accept_language=\"en\",\n",
        ").get_result()\n",
        "\n",
        "#Get Document Tones as a whole (market sentiment)\n",
        "doc_tone_df = json_normalize(data=tone_analysis[\"document_tone\"], record_path=[\"tones\"])\n",
        "\n",
        "# Sentences Tones (Get individual sentence sentement)\n",
        "sentences_tone_df = json_normalize(\n",
        "    data=tone_analysis[\"sentences_tone\"],\n",
        "    record_path=[\"tones\"],\n",
        "    meta=[\"sentence_id\", \"text\"],\n",
        ")\n",
        "print(f'Total Data points: {sentences_tone_df.shape}')\n",
        "print(sentences_tone_df.head(10))\n",
        "sentences_tone_df.to_csv('Data/Functionality/Twitter/sentences_tone.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Google (PyTrends):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Trends (PyTrends), analyzed in subsets of five due to function limitations. \n",
        "pytrends = TrendReq(hl= 'en-US')\n",
        "list_a = nlp_cryptos[0:5]\n",
        "list_b = nlp_cryptos[6:10]\n",
        "list_c = nlp_cryptos[11:15]\n",
        "list_d = nlp_cryptos[16:20]\n",
        "\n",
        "google_topics = []\n",
        "if len(list_a) > 0:\n",
        "    google_topics.append(list_a)\n",
        "\n",
        "if len(list_b) > 0:\n",
        "    google_topics.append(list_b)\n",
        "\n",
        "if len(list_c) > 0:\n",
        "    google_topics.append(list_c)\n",
        "\n",
        "if len(list_d) > 0:\n",
        "    google_topics.append(list_d)\n",
        "\n",
        "\n",
        "\n",
        "# List of cryptocurriences (only 5 crypto curriences at a time) \n",
        "# NOTE - pytrends gives 400 error if more then 5 cryptocurreinces are added. \n",
        "total_df = pd.DataFrame()\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "\n",
        "for list in google_topics:\n",
        "        pytrends.build_payload(list, cat=0, timeframe='today 3-m', gprop='news')\n",
        "        crypto_trends = pytrends.interest_over_time()\n",
        "        crypto_trends=crypto_trends.drop(columns= 'isPartial')        \n",
        "        total_df = crypto_trends.join(total_df).dropna(how=\"any\")\n",
        "        crypto_trends.tail(10)\n",
        "\n",
        "# Set current date and the date from one month ago using the ISO format\n",
        "current_date = pd.Timestamp(datetime.now(), tz=\"America/New_York\").isoformat()\n",
        "past_date = pd.Timestamp(datetime.now()- timedelta(30), tz=\"America/New_York\").isoformat()\n",
        "\n",
        "\n",
        "# Use newsapi client to get most relevant headlines per day in the past month\n",
        "def get_headlines(keyword):\n",
        "    all_headlines = []\n",
        "    all_dates = []    \n",
        "    date = datetime.strptime(current_date[:10], \"%Y-%m-%d\")\n",
        "    end_date = datetime.strptime(past_date[:10], \"%Y-%m-%d\")\n",
        "    print(f\"Fetching news about '{keyword}'\")\n",
        "    print(\"*\" * 30)\n",
        "    while date > end_date:\n",
        "        print(f\"retrieving news from: {date}\")\n",
        "        articles = newsapi.get_everything(\n",
        "            q=keyword,\n",
        "            from_param=str(date)[:10],\n",
        "            to=str(date)[:10],\n",
        "            language=\"en\",\n",
        "            sort_by=\"relevancy\",\n",
        "            page=1,\n",
        "        )\n",
        "        headlines = []\n",
        "        for i in range(0, len(articles[\"articles\"])):\n",
        "            headlines.append(articles[\"articles\"][i][\"title\"])\n",
        "        all_headlines.append(headlines)\n",
        "        all_dates.append(date)\n",
        "        date = date - timedelta(days=1)\n",
        "    return all_headlines, all_dates\n",
        "\n",
        "\n",
        "# Get first topic (kw = trending crypto)\n",
        "crypto_headlines, dates = get_headlines(\"crypto\")\n",
        "\n",
        "# inflation headline\n",
        "inflation_headlines, _ = get_headlines(\"inflation\")\n",
        "\n",
        "# Get third topic \n",
        "energy__headlines, _ = get_headlines(\"energy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Compute average compound sentiment of headlines for each day\n",
        "def headline_sentiment_summarizer_avg(headlines):\n",
        "    sentiment = []\n",
        "    for day in headlines:\n",
        "        day_score = []\n",
        "        for h in day:\n",
        "            if h == None:\n",
        "                continue\n",
        "            else:\n",
        "                day_score.append(sid.polarity_scores(h)[\"compound\"])\n",
        "        sentiment.append(sum(day_score) / len(day_score))\n",
        "    return sentiment\n",
        "\n",
        "# Get averages of each topics sentiment\n",
        "crypto_choice_avg = headline_sentiment_summarizer_avg(crypto_headlines)\n",
        "inflation_headlinese_avg = headline_sentiment_summarizer_avg(inflation_headlines)\n",
        "energy__headlines_avg = headline_sentiment_summarizer_avg(energy__headlines)\n",
        "\n",
        "\n",
        "# Combine Sentiment Averages into DataFrame\n",
        "topic_sentiments = pd.DataFrame(\n",
        "    {\n",
        "        \"crypto_choice_avg\": crypto_choice_avg,\n",
        "        \"inflation_headlinese_avg\": inflation_headlinese_avg,\n",
        "        \"energy_consumption_avg\": energy__headlines_avg,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Set the index value of the sentiment averages DataFrame to be the series of dates, Merge with main dataframe\n",
        "topic_sentiments.index = pd.to_datetime(dates)\n",
        "topic_sentiments_trends = total_df.join(topic_sentiments).dropna(how=\"any\")\n",
        "topic_sentiments_trends.to_csv('Data/Functionality/Google/Sentiments.csv')\n",
        "correlation_csv = topic_sentiments_trends.corr()\n",
        "correlation_csv.to_csv('Data/Functionality/Google/Correlation.csv')\n",
        "\n",
        "\n",
        "# Correlate the headlines' sentiment to returns\n",
        "correlation_df = topic_sentiments_trends.corr().style.background_gradient(cmap='PuBu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Finished Extracting NLP Data.. see Investor Summary')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Final_nlp_twitter.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
